# Proyecto: Pipeline ETL de Vuelos en Argentina

Este proyecto implementa un pipeline de datos completo para ingestar, procesar y analizar informes de vuelos dom칠sticos en Argentina, cubriendo el per칤odo de Enero 2021 a Junio 2022.

El pipeline utiliza un stack de herramientas de Big Data, incluyendo Apache Airflow para la orquestaci칩n, Apache Spark para el procesamiento ETL, y Apache Hive como Data Warehouse para el an치lisis final.

---

## 游늳 Arquitectura del Pipeline

El flujo de trabajo sigue este orden:

1.  **Orquestaci칩n (Apache Airflow):** Un DAG de Airflow (`proceso_aeropuertos_etl`) define y ejecuta las tareas en el orden correcto.
2.  **Ingesta (`ej_1_ingest_informes.sh`):** Un script de Bash descarga los archivos CSV de una fuente p칰blica y los transfiere al Data Lake en HDFS.
3.  **Procesamiento (Apache Spark):** Un script de PySpark (`process_aeropuertos.py`) lee los CSVs "crudos" de HDFS, aplica transformaciones (limpieza, renombrado, filtrado) y los guarda en tablas de Hive optimizadas.
4.  **Almacenamiento (Apache Hive):** Los datos limpios residen en un Data Warehouse de Hive, listos para ser consultados v칤a SQL.

---

## 游늭 Estructura del Proyecto

Este repositorio est치 organizado para documentar cada etapa del pipeline.
